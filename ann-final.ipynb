{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import ClassLabel\n",
    "\n",
    "# preprocess data\n",
    "df = pd.read_csv('aita_combined_verdicts.csv')\n",
    "\n",
    "label2id = {'not the asshole': 0, 'asshole': 1} \n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "df['labels'] = df['verdict'].map(label2id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f85d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each class in the 'labels' column\n",
    "class_counts = df['labels'].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "class_percentages = (class_counts / class_counts.sum()) * 100\n",
    "\n",
    "# Map the class indices to their corresponding class names\n",
    "class_percentages = class_percentages.rename(index=id2label)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=class_percentages.index, y=class_percentages.values, palette=\"viridis\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Percentage of Instances')\n",
    "plt.title('Class Distribution in Dataset (Percentages)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55076b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "# Separate majority and minority class\n",
    "df_majority = df[df['labels'] == 0]\n",
    "df_minority = df[df['labels'] == 1]\n",
    "\n",
    "# Oversample the minority class\n",
    "df_minority_oversampled = resample(df_minority, \n",
    "                                   replace=True, \n",
    "                                   n_samples=len(df_majority), \n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine the oversampled minority class with the majority class\n",
    "# if you don't want to oversample, comment out this block\n",
    "df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d25cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "# dif bert versions: \n",
    "model_name = 'prajjwal1/bert-medium'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['body'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_balanced[['body', 'labels']])\n",
    "\n",
    "# Convert the 'labels' column to ClassLabel type\n",
    "class_label = ClassLabel(num_classes=len(label2id), names=list(label2id.keys()))\n",
    "dataset = dataset.cast_column('labels', class_label)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "# ensure stratify_by_column='labels' for stratified splitting when using non oversampled data\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, stratify_by_column='labels', seed=10)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e2568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106680' max='106680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106680/106680 52:23, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.643838</td>\n",
       "      <td>0.771997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.653574</td>\n",
       "      <td>0.776004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1778' max='1778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1778/1778 01:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7760044995957395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'prajjwal1/bert-medium',\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "\n",
    "# evaluate accuracy\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# name of dir that you want to save stuff in (ex. bert_tiny, bert_tiny_oversampled)\n",
    "main_dir = \"bert_medium_0.01_dropout\"\n",
    "# num epochs\n",
    "num_epochs = 15\n",
    "# prob name something like \"model_oversampled_no_dropout_bert_tiny_epochs_{num_epochs}\"\n",
    "save_name = f\"model_oversampled_dropout_bert_medium_dropout_0.01_epochs_{num_epochs}\"\n",
    "\n",
    "#  by saving checkpoints, we can save models at different epochs and compare them :D\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{main_dir}/{save_name}\", # where to save checkpoints\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# After training, evaluate on the test set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Save evaluation results to a text file\n",
    "with open(f\"./{main_dir}/{save_name}_evals.txt\", \"w\") as file:\n",
    "    for key, value in eval_results.items():\n",
    "        file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Print the accuracy from the evaluation results\n",
    "print(f\"Test Accuracy: {eval_results['eval_accuracy']}\")\n",
    "    \n",
    "model.save_pretrained(f\"./{main_dir}/{save_name}\")\n",
    "# save tokenizer - optional\n",
    "# tokenizer.save_pretrained(f\"./{main_dir}/{save_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada75447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text to predict\n",
    "# Should be NTA\n",
    "text_not_asshole = \"\"\"AITAH for kicking out my gf's sister and her kids out of my flat after my gf gave her the keys \n",
    "\n",
    "People around me are saying I am an AH but I need the perspective of uninvolved people.\n",
    "My long term gf has my apartment keys, as I have hers. Only unspoken until now but always respected rule was, if you need to go to the other place, just send a text \"Hey going to your place\". Doesn't matter if the other is at home or even responds. Just simply to tell the other you will be at their place.\n",
    "\n",
    "I was supposed to be away 700km from home for 2 weeks for work related stuff, but 4 days in and our instructor got into an accident. Work tried to find another one, but no such luck on very short notice. They decided at like 10pm to get us the 1st flight home the next day at like 6am, pay us the overtime and the next day at home, then resume our normal work schedule.\n",
    "So I get home the next day at like 9am, sent a text to my gf to tell her I am back.\n",
    "Getting to my door, I am very confused hearing children screaming inside since none of the people who have my keys have low kids like that (my brother and my gf). I thought I got squatters or something. Opening the door and I see my gf's sister's kid running around after a shower, putting water everywhere. Plates of half finished ravioli on my living room ground. Their suitcases opened in the entrance.\n",
    "\n",
    "I get inside and see the husband on my couch trying to hook up my PS2(that he must have digged out in my storage room). Getting into a verbal argument with him trying to understand why the fuck they are here. Said my gf told them they could get my flat for 2 weeks while I was gone (they wanted to visit the city for a bit, go to the beach). My gf sent me a text while i was arguing, telling me \"oh ok, btw my sis fam' is at your flat\".\n",
    "\n",
    "I admit I blew up on him and the sis who left my bedroom in the meantime. Told them to leave immediately. They argued quite a bit, my gf called her sis, then sis put up the speaker so we could all hear, and she said I was embarrassing her, that she told them they could use my place for a while.\n",
    "\n",
    "I threatened to call the police, also asked my brothers to come.\n",
    "They left while cursing me to their children, telling that holidays are over because the mean little sister's boyfriend cast us out.\n",
    "I have now all of my gf family on my back, and even some of my own family, saying i could have stayed with at my gf so the kids could have some vacations...\n",
    "\n",
    "Also. They have read my doctor prescription papers(I put them in a specific order, and it was not the same), and obviously took some of my prescribed meds (opened a box of benzodiazepine...).\n",
    "\n",
    "AITAH for making them leave? We pretty much stopped talking about anything else with my gf. I feel like i am being gaslighted. I would never invite people to her apartment like that, especially without telling her. It seems so disrespectful.\n",
    "Am I going insane?\n",
    "\"\"\"\n",
    "\n",
    "# List of texts to predict\n",
    "texts = [ text_not_asshole ]\n",
    "\n",
    "# Tokenize the texts as a batch\n",
    "# If not using NVIDIA/CUDA, comment out the .to(device) part\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_classes = logits.argmax(dim=-1).tolist()\n",
    "\n",
    "# Map predicted labels to their corresponding class names\n",
    "predicted_labels = [id2label[pred] for pred in predicted_classes]\n",
    "\n",
    "# Print the predictions\n",
    "for text, label in zip(texts, predicted_labels):\n",
    "    print(f\"Text: {text[:50]}...\") \n",
    "    print(f\"Predicted class: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
